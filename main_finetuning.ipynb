{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import glob\n",
    "from bisenet import BiSeNet\n",
    "from dataset import CustomDataset, display_batch, display_segmentation, display_result\n",
    "from loss import OhemCELoss, BCELoss2d, DiceLoss, CrossEntropyLoss2d, NLLLoss2d\n",
    "from metrics import IoU_score\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from colorama import init\n",
    "init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms to the ImageFolder structure\n",
    "data_root = \"C:\\\\Users\\\\gueganj\\\\Desktop\\\\My_DataBase\\\\shuang_data\\\\\"\n",
    "# Models\n",
    "model_name = \"unet\"\n",
    "# Number of classes in the dataset\n",
    "num_classes = 1\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "# Learning rate\n",
    "lr = 0.05\n",
    "# Momentum\n",
    "momentum = 0.99\n",
    "# Number of epochs to train for \n",
    "num_epochs = 100\n",
    "# prediction threshold\n",
    "threshold = 0.25\n",
    "# size of image in input\n",
    "input_size = 224\n",
    "# total number of image used\n",
    "size_dataset = 100\n",
    "# Flag for feature extracting. When False, we finetune the whole model, when True we only update the reshaped layer params\n",
    "feature_extract = False\n",
    "# Flag for using Tensorboard tool\n",
    "use_tensorboard = True\n",
    "# Flag for using data augmentation\n",
    "use_augmentation = False\n",
    "# Flag for using a learning rate scheduler\n",
    "use_scheduler = \"cosine\"\n",
    "# Load checkpoint\n",
    "load_checkpoint = False\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826a87cfb9f849fc877b6cefb187214c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatLogSlider(value=8.0, base=2.0, description='Batch Size', max=10.0, min=2.0, readout_format='d', step=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6829a4f5230f4ff28c05f193b71126b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatLogSlider(value=0.001, description='Learning Rate', max=2.0, min=-6.0, readout_format='.1g', step=1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56f9814fe7c4c9faa84b2655cb3ade0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='LR scheduler', options=('Constant', 'Cosine', 'Exponential', 'Reduce On Plateau'), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d6ed608bad4d6b8ea1dd33aebc52b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatLogSlider(value=128.0, base=2.0, description='Image Size', max=10.0, min=7.0, readout_format='d', step=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6f542620c84d49850ebaf523b49248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=100, description='Dataset Size', max=1000, min=100, step=100)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da852e2f00c4aa8a37c400faeae3faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=100, description='Number of Epochs', max=1000, min=10, step=10, style=SliderStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96d2bc138fb44908a3abd926110b810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.9, description='Momentum', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847f02f7a34f4ac4a1f6e8296d6485ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Use Data Augmentation', options=('yes', 'no'), style=DescriptionStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab4b6d0cfca46e89a239642f05a0ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Use Tensorboard', options=('yes', 'no'), style=DescriptionStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f541167368b4fa6b06b477e390ea66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Optimizer', options=('SGD', 'Adam', 'RMSprop', 'ASGD', 'Adamax', 'Adagrad', 'Adadel…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba5390a83254c2bbe205e31c1273e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Use Tensorboard')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "w1 = widgets.FloatLogSlider(value=8, base=2, min=2, max=10, step=1, description='Batch Size', readout_format='d')\n",
    "w2 = widgets.FloatLogSlider(value=0.001, base=10, min=-6, max=2, step=1, description='Learning Rate', readout_format='.1g')\n",
    "w3 = widgets.ToggleButtons(options=['Constant', 'Cosine', 'Exponential', 'Reduce On Plateau'], description='LR scheduler')\n",
    "w4 = widgets.FloatLogSlider(value=8, base=2, min=7, max=10, step=1, description='Image Size', readout_format='d')\n",
    "w5 = widgets.IntSlider(value=100, min=100, max=1000, step=100, description='Dataset Size', readout_format='d')\n",
    "w6 = widgets.IntSlider(value=100, min=10, max=1000, step=10, description='Number of Epochs', readout_format='d', style = {'description_width': 'initial'})\n",
    "w7 = widgets.FloatSlider(value=0.9, min=0, max=1, step=0.01, description='Momentum')\n",
    "w8 = widgets.RadioButtons(options=['yes', 'no'], description='Use Data Augmentation', style = {'description_width': 'initial'})\n",
    "w9 = widgets.RadioButtons(options=['yes', 'no'], description='Use Tensorboard', style = {'description_width': 'initial'})\n",
    "w0 = widgets.ToggleButtons(options=['SGD', 'Adam', 'RMSprop', 'ASGD', 'Adamax', 'Adagrad', 'Adadelta'], description='Optimizer')\n",
    "display(w1,w2,w3,w4,w5,w6,w7,w8,w9,w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and Reshape the Networks\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "if model_name == \"bisenet\":\n",
    "    # Load\n",
    "    file_path  = 'C:\\\\Users\\\\gueganj\\\\Desktop\\\\face parsing - PyTorch\\\\res\\\\cp\\\\79999_iter.pth'\n",
    "    model = BiSeNet(n_classes=19) # trained on 19 classes\n",
    "    model.load_state_dict(torch.load(file_path, map_location=device))\n",
    "    # change final layer to tune and output only 2 classes\n",
    "    set_parameter_requires_grad(model, feature_extract)\n",
    "    model.conv_out.conv_out   = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    model.conv_out16.conv_out = nn.Conv2d(64, num_classes, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    model.conv_out32.conv_out = nn.Conv2d(64, num_classes, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "elif model_name == \"unet\":\n",
    "    import segmentation_models_pytorch as smp\n",
    "    model = smp.Unet(encoder_name='mobilenet_v2', activation=None) # Activation=None because I apply activation layer myself\n",
    "    model.segmentation_head[0] = nn.Conv2d(16, num_classes, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "if use_augmentation:\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize((input_size, input_size)),\n",
    "                    transforms.RandomAffine(\n",
    "                        degrees=10,\n",
    "                        translate=(0.1, 0.1),\n",
    "                        scale=(0.9, 1.1),\n",
    "                        resample=2,\n",
    "                        shear=5),\n",
    "                    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.05),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor()])\n",
    "                    norm_mean\n",
    "                    norm_std\n",
    "                    #transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])])\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize((input_size, input_size)),\n",
    "                    transforms.ToTensor()])\n",
    "    \n",
    "# path\n",
    "folder_data = glob.glob(os.path.join(data_root,\"images\\\\*.png\"))\n",
    "folder_mask = glob.glob(os.path.join(data_root,\"masks\\\\*.png\"))\n",
    "# suffle the 2 lists the same way (to be sure)\n",
    "lists_shuffled = list(zip(folder_data, folder_mask))\n",
    "random.shuffle(lists_shuffled)\n",
    "folder_data, folder_mask = zip(*lists_shuffled)\n",
    "# split in train/test\n",
    "train_size   = 0.8\n",
    "train_image_paths = folder_data[:int(size_dataset*train_size)]\n",
    "test_image_paths  = folder_data[int(size_dataset*train_size):]\n",
    "train_mask_paths  = folder_mask[:int(size_dataset*train_size)]\n",
    "test_mask_paths   = folder_mask[int(size_dataset*train_size):]\n",
    "# create DataLoader\n",
    "train_dataset = CustomDataset(train_image_paths, train_mask_paths, transform)\n",
    "test_dataset  = CustomDataset(test_image_paths, test_mask_paths)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader   = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-269be1e9ef30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Gather the parameters to be optimized/updated in this run : finetuning or feature extract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mparams_to_update\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Parameters to learn:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mfeature_extract\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mparams_to_update\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Gather the parameters to be optimized/updated in this run : finetuning or feature extract\n",
    "params_to_update = model.parameters()\n",
    "print(\"Parameters to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "if algo_optim == 'SGD':\n",
    "    optimizer_ft = optim.SGD(params_to_update, lr, momentum)\n",
    "elif algo_optim == 'Adam':\n",
    "    optimizer_ft = optim.Adam(params_to_update, lr=lr)\n",
    "elif algo_optim == 'RMSprop':\n",
    "    optimizer_ft = optim.RMSprop(params_to_update, lr=lr)\n",
    "elif algo_optim == 'ASGD':\n",
    "    optimizer_ft = optim.ASGD(params_to_update, lr=lr)\n",
    "elif algo_optim == 'Adamax':\n",
    "    optimizer_ft = optim.Adamax(params_to_update, lr=lr)\n",
    "elif algo_optim == 'Adagrad':\n",
    "    optimizer_ft = optim.Adagrad(params_to_update, lr=lr)\n",
    "elif algo_optim == 'Adadelta':\n",
    "    optimizer_ft = optim.Adadelta(params_to_update, lr=lr)\n",
    "                  \n",
    "if use_scheduler:\n",
    "    if use_scheduler=='cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, len(train_loader))\n",
    "    elif use_scheduler=='exponential':\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer_ft, gamma=1.5)\n",
    "    else:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss() #BCELoss2d() #DiceLoss() #CrossEntropyLoss2d() #DiceLoss() #NLLLoss2d\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = datetime.now().strftime(\"%d_%m_%Y-%H_%M\")\n",
    "if use_tensorboard:\n",
    "    writer    = SummaryWriter('tensorboard_logs/' + date_time)\n",
    "    # to do : configure max_queue to SummaryWriter()\n",
    "    images, labels = iter(train_loader).next()\n",
    "    img_grid = utils.make_grid(images, nrow=4, padding=10)\n",
    "    lbl_grid = utils.make_grid(labels.unsqueeze(1), nrow=4, padding=10)\n",
    "    writer.add_image('Images batch', img_grid)\n",
    "    writer.add_image('Labels batch', lbl_grid)\n",
    "    writer.add_graph(model, images)\n",
    "    writer.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_checkpoint:\n",
    "    checkpoint = torch.load(load_checkpoint)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer_ft.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss  = checkpoint['loss']\n",
    "    score = checkpoint['IoU_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, criterion, num_epochs=25, threshold=0.5):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    best_acc   = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            # Iterate over data.\n",
    "            running_loss, running_iou = 0.0, 0.0\n",
    "            for i, (inputs, labels) in enumerate(tqdm(dataloaders[phase])):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # FORWARD\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    output = model(inputs)\n",
    "                    logits = output.squeeze(1).detach()\n",
    "                    predictions = torch.sigmoid(logits) > threshold\n",
    "                    loss = criterion(output.squeeze(1), labels)\n",
    "                    \n",
    "                    # BACKWARD (in training phase)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        if use_scheduler:\n",
    "                            scheduler.step()\n",
    "                        \n",
    "                # statistics\n",
    "                running_loss += loss.item() #* inputs.size(0)\n",
    "                running_iou  += IoU_score(predictions, labels)\n",
    "                if use_tensorboard and (i%int(batch_size)==0):\n",
    "                    writer.add_scalar('loss/'+phase, running_loss/(i+1), 1+i+epoch*len(dataloaders[phase]))\n",
    "                    writer.add_scalar('score/'+phase, running_iou/(i+1), 1+i+epoch*len(dataloaders[phase]))\n",
    "                    inputs_0  = inputs[0,:,:,:].permute(1,2,0)\n",
    "                    predict_0 = predictions[0,:,:]\n",
    "                    proba_0   = torch.sigmoid(logits[0,:,:])\n",
    "                    label_0   = labels[0,:,:]\n",
    "                    writer.add_figure('Result', display_result(inputs_0, label_0, predict_0, proba_0), global_step=1+i+epoch*len(dataloaders[phase]))\n",
    "                    if use_scheduler:\n",
    "                        writer.add_scalar('lr/'+phase, scheduler.get_last_lr()[0], 1+i+epoch*len(dataloaders[phase]))\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase])\n",
    "            epoch_iou  = running_iou  / len(dataloaders[phase])\n",
    "\n",
    "            tqdm.write('{:5s} : Loss={:.4f} - IoU={:.4f}'.format(phase, epoch_loss, epoch_iou))\n",
    "\n",
    "                \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_iou > best_acc:\n",
    "                best_acc   = epoch_iou\n",
    "                best_model = copy.deepcopy(model)\n",
    "                ckpt_path  = 'checkpoint_logs/'+date_time+'.ckpt'\n",
    "                torch.save({'epoch':epoch,'model':best_model.state_dict(),'optimizer':optimizer.state_dict(),'loss':epoch_loss,'IoU_score':epoch_iou}, ckpt_path)\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_iou)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Accuracy: {:4f}'.format(best_acc))\n",
    "    \n",
    "    return best_model, best_acc\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# loaders dict\n",
    "dataloaders_dict = {}\n",
    "dataloaders_dict['train'] = train_loader\n",
    "dataloaders_dict['val']   = test_loader\n",
    "\n",
    "# Train and evaluate \n",
    "model_ft, best_acc = train_model(model, dataloaders_dict, optimizer_ft, criterion, num_epochs, threshold)\n",
    "\n",
    "\n",
    "writer.add_hparams({\"Image size\":int(input_size),\n",
    "                   \"shuang/Julien\":\"shuang\",\n",
    "                   \"Dataset size\":int(size_dataset),\n",
    "                   \"Architecture\":model_name,\n",
    "                   \"Learning rate\":lr,\n",
    "                   \"Momentum\":momentum,\n",
    "                   \"LR scheduler\": scheduler.__class__.__name__ if use_scheduler else 'None',\n",
    "                   \"Optimisation algorithm\":optimizer_ft.__class__.__name__,\n",
    "                   \"Epoch number\":int(num_epochs),\n",
    "                   \"Batch size\":int(batch_size),\n",
    "                   \"Loss\":criterion.__class__.__name__,\n",
    "                   \"Threshold sigmoid\":threshold}, \n",
    "                   {'hparam/IoU score':best_acc})\n",
    "\n",
    "'''\n",
    "score_thres = 0.7\n",
    "ignore_idx  = -100\n",
    "n_min = 16 * 448 * 448//16\n",
    "LossP = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n",
    "Loss2 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n",
    "Loss3 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n",
    "#lossp, loss2, loss3 = LossP(out, labels), Loss2(out16, labels), Loss3(out32, labels)\n",
    "#loss                = lossp + loss2 + loss3\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
