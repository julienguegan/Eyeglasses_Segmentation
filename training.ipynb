{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\gueganj\\\\Desktop\\\\Eyeglasses Detection'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('..')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import numpy as np\n",
    "from models.bisenet import BiSeNet\n",
    "from models.pranet import PraNet\n",
    "from dataset import Dataset_SMP, get_preprocessing, get_training_augmentation, split_data\n",
    "from display import display_result, display_proba\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import segmentation_models_pytorch as smp\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(123456)\n",
    "torch.manual_seed(123456)\n",
    "torch.cuda.manual_seed(123456)\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms to the ImageFolder structure\n",
    "data_root = \"C:\\\\Users\\\\gueganj\\\\Desktop\\\\My_DataBase\\\\nature\\\\face_cropped\\\\\"\n",
    "# Models\n",
    "model_name = \"unet\"\n",
    "# encoder\n",
    "encoder = \"mobilenet_v2\"\n",
    "# Number of classes in the dataset\n",
    "num_classes = 1\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 8\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "# Momentum\n",
    "momentum = 0.99\n",
    "# Weight decay\n",
    "weight_decay = 0.0001\n",
    "# Nesterov\n",
    "nesterov = True\n",
    "# algorithm use for optimization\n",
    "algo_optim = 'SGD'\n",
    "# Number of epochs to train for \n",
    "num_epochs = 200\n",
    "# prediction threshold\n",
    "threshold = 0.5\n",
    "# size of image in input\n",
    "input_size = [256,256] #[544,960] #[448,448] # multiple of 32 [544,960]\n",
    "# total number of image used\n",
    "size_dataset = 1000\n",
    "# Flag for feature extracting. When False, we finetune the whole model, when True we only update the reshaped layer params\n",
    "feature_extract = False\n",
    "# Flag for using Tensorboard tool\n",
    "use_tensorboard = True\n",
    "# Flag for using data augmentation\n",
    "use_augmentation = True\n",
    "# Flag for using a learning rate scheduler\n",
    "lr_scheduler = \"constant\"\n",
    "# Load checkpoint\n",
    "load_checkpoint = False #\"C:\\\\Users\\\\gueganj\\\\Desktop\\\\Eyeglasses Detection\\\\checkpoint_logs\\\\27_10_2020-11_50.ckpt\"\n",
    "# Landmarks directory\n",
    "landmarks_dir = False # os.path.join(data_root,\"landmarks\",\"face_landmarks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config dictionnary\n",
    "config = {}\n",
    "for item in dir().copy():\n",
    "    if (not item.startswith('_')) and (item!='In') and (item!='Out') and item != 'config' and item != 'item':\n",
    "        if str(type(eval(item)))[8:-2] in ['str','bool','int','float']:\n",
    "            config[item] = eval(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_classes == 1:\n",
    "    activation = 'sigmoid'\n",
    "else:\n",
    "    activation = 'softmax'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,628,816 parameters to learn\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"bisenet\":\n",
    "    # Load\n",
    "    model = BiSeNet(n_classes, activation) # trained on 19 classes\n",
    "    model.conv_out.conv_out   = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    #model.conv_out16.conv_out = nn.Conv2d(64, num_classes, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    #model.conv_out32.conv_out = nn.Conv2d(64, num_classes, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    if landmarks_dir:\n",
    "        model.cp.resnet.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "elif model_name == \"unet\":\n",
    "    model = smp.Unet(encoder_name=encoder,  encoder_weights='imagenet', activation=activation)\n",
    "    model.segmentation_head[0] = nn.Conv2d(16, num_classes, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "    if landmarks_dir:\n",
    "        model.encoder.features[0][0] = nn.Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "elif model_name == \"DeepLabV3Plus\":\n",
    "    model = smp.DeepLabV3Plus(encoder_name=encoder,  encoder_weights='imagenet', activation=activation)\n",
    "    model.segmentation_head[0] = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "\n",
    "elif model_name == \"pranet\":\n",
    "    model = PraNet()\n",
    "    model.ra2_conv4.conv = nn.Conv2d(64, num_classes, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.ra2_conv4.bn   = nn.BatchNorm2d(num_classes, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    \n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(encoder, 'imagenet')\n",
    "model.to(device)\n",
    "\n",
    "print('{:,} parameters to learn'.format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL : 119  images -  47  personnes\n",
      "train : 92  images -  37  personnes\n",
      "valid : 10  images -  4  personnes\n",
      "test  : 17  images -  6  personnes\n"
     ]
    }
   ],
   "source": [
    "# split in train/test\n",
    "train_set, valid_set, test_set = split_data(data_root, \"images\", \".jpg\", \"masks\\\\frame\", \".png\", size_dataset, use_id=True)\n",
    "train_image, train_mask = train_set\n",
    "valid_image, valid_mask = valid_set\n",
    "test_image, test_mask   = test_set\n",
    "# augmentation\n",
    "if use_augmentation:\n",
    "    train_augmentation = get_training_augmentation()\n",
    "else:\n",
    "    train_augmentation = None\n",
    "# create DataLoader\n",
    "train_dataset = Dataset_SMP(train_image, train_mask, input_size, train_augmentation, get_preprocessing(preprocessing_fn), landmarks_dir)\n",
    "valid_dataset = Dataset_SMP(valid_image, valid_mask, input_size, None, get_preprocessing(preprocessing_fn), landmarks_dir)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(image, preprocessing_fn):\n",
    "    image = image * torch.tensor(preprocessing_fn.keywords['std']).view(-1,1,1) + torch.tensor(preprocessing_fn.keywords['mean']).view(-1,1,1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ==> all 6,628,816 parameters to learn\n"
     ]
    }
   ],
   "source": [
    "n_param_total = sum(p.numel() for p in model.parameters())\n",
    "if feature_extract:\n",
    "    print(\"Parameters to learn : \")\n",
    "    params_to_update = []\n",
    "    params_to_finetuned = re.compile('(segmentation_head.*)|(decoder.blocks.[4].*.)')\n",
    "    for name, param in model.named_parameters():\n",
    "        if params_to_finetuned.match(name):\n",
    "            print(2*\"\\t\", \" - \", name)\n",
    "            params_to_update.append(param)\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    n_param_extract = sum(p.numel() for p in params_to_update)\n",
    "    print('\\n ==> {:,}/{:,} = {:.2f} % parameters to learn'.format(n_param_extract,n_param_total,100*(n_param_extract/n_param_total)))  \n",
    "else:\n",
    "    params_to_update = model.parameters()\n",
    "    print('\\n ==> all {:,} parameters to learn'.format(n_param_total))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algo_optim == 'SGD':\n",
    "    optimizer_ft = optim.SGD(params_to_update, lr, momentum, nesterov = True)\n",
    "elif algo_optim == 'Adam':\n",
    "    optimizer_ft = optim.Adam(params_to_update, lr=lr)\n",
    "elif algo_optim == 'RMSprop':\n",
    "    optimizer_ft = optim.RMSprop(params_to_update, lr=lr)\n",
    "elif algo_optim == 'ASGD':\n",
    "    optimizer_ft = optim.ASGD(params_to_update, lr=lr)\n",
    "elif algo_optim == 'Adamax':\n",
    "    optimizer_ft = optim.Adamax(params_to_update, lr=lr)\n",
    "elif algo_optim == 'Adagrad':\n",
    "    optimizer_ft = optim.Adagrad(params_to_update, lr=lr)\n",
    "elif algo_optim == 'Adadelta':\n",
    "    optimizer_ft = optim.Adadelta(params_to_update, lr=lr)\n",
    "                  \n",
    "if lr_scheduler=='cosine':\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer_ft, len(train_loader))\n",
    "elif lr_scheduler=='exponential':\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer_ft, gamma=1.5)\n",
    "elif lr_scheduler=='reduceOnPlateau':\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft)\n",
    "elif lr_scheduler=='constant':\n",
    "    scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight  = 20 * torch.ones((batch_size, 1, 1, 1))\n",
    "loss    = smp.utils.losses.DiceLoss() # BCEDiceLoss() # smp.utils.losses.CrossEntropyLoss() #  #  # smp.utils.losses.BCELoss(weight=weight)# \n",
    "metrics = [smp.utils.metrics.IoU(threshold=threshold)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 256, 256] at entry 0 and [1, 256, 256] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-eb0d0a594bc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mwriter\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tensorboard_logs/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdate_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# to do : configure max_queue to SummaryWriter()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlandmarks_dir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mlandmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# scalars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 256, 256] at entry 0 and [1, 256, 256] at entry 1"
     ]
    }
   ],
   "source": [
    "date_time = datetime.now().strftime(\"%d_%m_%Y-%H_%M\")\n",
    "if use_tensorboard:\n",
    "    writer    = SummaryWriter('tensorboard_logs/' + date_time)\n",
    "    # to do : configure max_queue to SummaryWriter()\n",
    "    images, labels = iter(train_loader).next()\n",
    "    if landmarks_dir:\n",
    "        landmarks = images[:,3,:,:].unsqueeze(1)\n",
    "        images    = images[:,:3,:,:]\n",
    "        lnd_grid  = utils.make_grid(landmarks, nrow=4, padding=10)\n",
    "        writer.add_image('Landmarks batch', lnd_grid)\n",
    "    if len(labels.shape)==4 and labels.shape[1]==4:\n",
    "        labels = np.abs(1-labels[:,0,:,:].unsqueeze(1))\n",
    "    if len(labels.shape)==4 and labels.shape[1]==3:\n",
    "        labels = torch.sum(labels,dim=1).unsqueeze(1)\n",
    "    images   = denormalize(images, preprocessing_fn)\n",
    "    img_grid = utils.make_grid(images, nrow=4, padding=10, scale_each=True)\n",
    "    lbl_grid = utils.make_grid(labels, nrow=4, padding=10)\n",
    "    writer.add_image('Images batch', img_grid)\n",
    "    writer.add_image('Labels batch', lbl_grid)\n",
    "    writer.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_checkpoint:\n",
    "    checkpoint = torch.load(load_checkpoint, map_location=device)\n",
    "    # my own checkpoint\n",
    "    if 'model' in checkpoint.keys():\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer_ft.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch      = checkpoint['epoch']\n",
    "        loss_value = checkpoint['loss']\n",
    "        iou_score  = checkpoint['iou_score']\n",
    "    # open source checkpoint\n",
    "    else: \n",
    "        model.load_state_dict(checkpoint)\n",
    "    print(\"checkpoint loaded !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create epoch runners, it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(model, loss=loss, metrics=metrics, optimizer=optimizer_ft, device=device, verbose=True)\n",
    "valid_epoch = smp.utils.train.ValidEpoch(model, loss=loss, metrics=metrics, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_0 = iter(valid_loader).next()[0][0,:,:,:].unsqueeze(0)\n",
    "if landmarks_dir:\n",
    "    inputs_0 = inputs_0[:,:3,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 40 epochs\n",
    "best_score = 0\n",
    "for epoch in range(1, num_epochs):\n",
    "    \n",
    "    print('\\n {} Epoch {}/{} {}'.format('=' * 20, epoch, num_epochs, '=' * 20))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "\n",
    "    if use_tensorboard:\n",
    "        writer.add_scalar('loss/val', valid_logs['dice_loss'], epoch)\n",
    "        writer.add_scalar('score/val', valid_logs['iou_score'], epoch)\n",
    "        writer.add_scalar('loss/train', train_logs['dice_loss'], epoch)\n",
    "        writer.add_scalar('score/train', train_logs['iou_score'], epoch)\n",
    "        # do a prediction to display\n",
    "        with torch.no_grad():\n",
    "            prediction = model.forward(inputs_0.to(device)).cpu()\n",
    "            images = denormalize(inputs_0.squeeze(0), preprocessing_fn)\n",
    "            writer.add_figure('Result', display_proba(images.permute(1,2,0), prediction), global_step=epoch)\n",
    "\n",
    "    # save model\n",
    "    if best_score < valid_logs['iou_score']:\n",
    "        best_score = valid_logs['iou_score']\n",
    "        best_model = copy.deepcopy(model)\n",
    "        ckpt_path  = 'checkpoint_logs/'+date_time+'.ckpt'\n",
    "        torch.save({'config':config,'epoch':epoch,'model':best_model.state_dict(),'optimizer':optimizer_ft.state_dict(),'loss':valid_logs[loss.__name__],'iou_score':valid_logs['iou_score']}, ckpt_path)\n",
    "        print('Model saved!')\n",
    "        if use_tensorboard:\n",
    "            writer.add_hparams(config, {'hparam/IoU score':best_score}, run_name='config')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test best saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset_SMP(test_image, test_mask, input_size, augmentation=None, preprocessing=get_preprocessing(preprocessing_fn))\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on test set\n",
    "metrics    = [smp.utils.metrics.IoU(threshold=0.15)]\n",
    "test_epoch = smp.utils.train.ValidEpoch(model=best_model, loss=loss, metrics=metrics, device=device)\n",
    "logs       = test_epoch.run(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
